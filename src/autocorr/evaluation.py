# Utilities for evaluation
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
# Utilities for annotation file corrections
import numpy as np
import pandas as pd
import json
import os, sys
# Utilities for logging
from src.controller import d2_mask

class AutoCorrEvaluator(object):

    def __init__(self, anno_gt_file, anno_dt_file):
        self.coco_gt = COCO(anno_gt_file)
        # self.coco_dt = self.coco_gt.loadRes(anno_dt_file)
        self.coco_dt = COCO(anno_dt_file)
        self._hack_coco_dt()

    def _hack_coco_dt(self):
        # inferred file from Mask-R-CNN has score. 
        # YOLOv7 doesn't support exporting the score in annotation files (although the score is included in the prediction tensor det[:,4] in predict.py)
        if 'score' in self.coco_dt.dataset['annotations']: pass
        else:
            for ann in self.coco_dt.dataset['annotations']:
                ann['score'] = 1.0
        
        # the ground truths (after editing in CVAT) don't have scores
        for anno in self.coco_gt.dataset['annotations']:
            anno['score'] = 1.0

    def evaluate(self, iou_type='segm', iou_Thrs=[]):
        coco_eval = COCOeval(self.coco_gt, self.coco_dt, iou_type)
        if iou_Thrs:
            coco_eval.params.iouThrs = iou_Thrs
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()
        coco_eval.summarize_per_category()
        return coco_eval

class AutoCorrEval:
    def __init__(self,
        out_dir="../../../SupContrast/output",
        gt_file = "",
        dt_file = "",
        end_eval_file = "",
        config_file="COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
        ):
        self.out_dir = out_dir
        self.gt_file = gt_file
        self.dt_file = dt_file
        self.cfg = config_file
        self.end_eval_file = end_eval_file
        logger, cfg = d2_mask.startup(regist_instances=False, cfg=self.cfg)
        self.logger = logger

    # Functions for components
    def findCategory(self, data):
        # find categories
        cats = data["categories"]
        category = pd.DataFrame(cats)
        category = category.drop(['supercategory'], axis=1)
        category = category.rename(columns={'id': 'category_id'})
        return category

    def findImages(self, data):
        img = data["images"]
        images = pd.DataFrame(img)
        # unwanted columns exist if exported from CVAT. Not if generated by my code
        if set(['license','flickr_url','coco_url','date_captured']).issubset(images.columns):
            images = images.drop(columns=['license','flickr_url','coco_url','date_captured'])
        return images

    def findAnnotations(self, data):
        anno = data["annotations"]
        df = pd.DataFrame(anno)
        return df

    # convert all np.integer, np.floating and np.ndarray into json recognisable int, float and lists
    class NpEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            if isinstance(obj, np.floating):
                return float(obj)
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            return json.JSONEncoder.default(self, obj)

    def drop_columns_if_exist(self, df, columns):
        df = df.copy()
        for col in columns:
            if col in df.columns:
                df = df.drop(columns=col)
        return df

    def createDF(self, filename):
        with open(filename, 'r') as file:
            data = json.load(file)
            
            category = self.findCategory(data)
            images = self.findImages(data)
            nos_image = images['id'].max()
            df = self.findAnnotations(data)
            df = df.merge(images[['id','file_name']], left_on='image_id', right_on='id')
            df = df.rename(columns={'id_x': 'id'})
            df = self.drop_columns_if_exist(df,columns=['iscrowd','attributes','id_y'])
            return category, images, df
    
    def fix_image_id(self, image1, image2, df, df2):
        
        image_comb = pd.concat([image1, image2], ignore_index = True)                              # combine image 1 and 2
        # sort and find unique image names
        image_comb = image_comb.sort_values(by=['file_name']).reset_index(drop=True)               # sort by image name
        image_new = image_comb.drop_duplicates(subset=['file_name'])                               # Get unique image names
        image_new = image_new.reset_index(drop=True)                                               # reset index
        image_new['id'] = image_new.index + 1                                                      # create new image id

        image1 = image1.merge(image_new[['id', 'file_name']], on='file_name', how='left')        # map image_id to image1 and image2
        image2 = image2.merge(image_new[['id', 'file_name']], on='file_name', how='left')        # map image_id to image1 and image2

        # use new id in image to replace old image_id in df
        df = df.merge(image1[['id_x', 'id_y']], left_on='image_id', right_on='id_x', how='left')
        df2 = df2.merge(image2[['id_x', 'id_y']], left_on='image_id', right_on='id_x', how='left')
        df = df.drop(columns=['image_id', 'id_x']).rename(columns={'id_y': 'image_id'})
        df2 = df2.drop(columns=['image_id', 'id_x']).rename(columns={'id_y': 'image_id'})

        # Make good the dfs
        df['iscrowd'] = 0
        df['attributes'] = [{'occluded':False}] * len(df['id'])
        df = df.drop(columns=['file_name'])
        df2['iscrowd'] = 0
        df2['attributes'] = [{'occluded':False}] * len(df2['id'])
        df2 = df2.drop(columns=['file_name'])

        return image_new, df, df2

    # Align metadata of dt_file with gt_file
    def align_metadata(self):

        logger = self.logger
        # Store categories, images and annotations in separate dataframes
        category, images, df = self.createDF(self.gt_file)
        category2, images2, df2 = self.createDF(self.dt_file)

        # Check categories
        for i in range(len(category['name'])):
            if category['name'][i] != category2['name'][i]:
                logger.info('category id: {} , {} in file 1 different from category id: {} , {} in file 2. Please check'.format(category['category_id'][i], category['name'][i], category2['category_id'][i], category2['name'][i]))
        category = category.rename(columns={'category_id': 'id'})                           # clean category for json dump
        category['supercategory'] = ""

        # # Check numbers of images - does the detection file contain fewer images than the ground truth file?
        # DT may have more images than the GT because the GT only records positive images. DT may have predictions on images that are negative on GT.
        # nos_image = len(images['id'])
        # nos_image2 = len(images2['id'])
        # if nos_image2 > nos_image:
        #     logger.info(f"detection file contains {nos_image2} images and the ground truth only contains {nos_image}. Check file")
        # else:
        #     logger.info("Number of images OK!")
        
        # Change image id in dt_file to the one of gt_file
        # for i in range(len(df2['id'])):
        #     df2.loc[i, 'image_id'] = images.loc[(images['file_name']==df2['file_name'][i]), 'id'].values
        images_new, df, df2 = self.fix_image_id(images, images2, df, df2)

        # JSON with revised image_id exported for evaluation
        dict_to_gt = {
            "categories": category.to_dict('records'),
            "images": images_new.to_dict('records'),
            "annotations": df.to_dict('records')
            }
        dict_to_dt = {
            "categories": category.to_dict('records'),
            "images": images_new.to_dict('records'),
            "annotations": df2.to_dict('records')
            }
        with open(os.path.join(self.out_dir,"labels/gt_corrected.json"), "w") as outfile:
            json.dump(dict_to_gt, outfile, cls=self.NpEncoder)
        with open(os.path.join(self.out_dir,"labels/dt_corrected.json"), "w") as outfile:
            json.dump(dict_to_dt, outfile, cls=self.NpEncoder)
    
    def traditional_evaluate(self):
        gt_file = os.path.join(self.out_dir,"labels/gt_corrected.json")
        dt_file = os.path.join(self.out_dir,"labels/dt_corrected.json")
        eval = AutoCorrEvaluator(anno_gt_file=gt_file, anno_dt_file=dt_file)
        result = eval.evaluate()

        # Parse results as a DataFrame
        # Get titles of metrics
        metrics = list(["AP","AP@.50IOU","AP@.75IOU","AP (small)","AP (medium)","AP (large)","AR@1","AR@10","AR@100","AR@100 (small)","AR@100 (medium)","AR@100 (large)"])
        # Assemble Dataframe
        stats = [list(i) for i in zip(*result.category_stats)]
        assessed = pd.DataFrame(stats, columns=metrics)
        # Copy categories
        category = self.createDF(self.gt_file)[0]
        assessed['category'] = category['name']
        metrics.insert(0,'category')
        assessed = assessed.reindex(columns = metrics)

        return assessed
    
    def rangeIoU_evaluate(self):
        gt_file = os.path.join(self.out_dir,"labels/gt_corrected.json")
        dt_file = os.path.join(self.out_dir,"labels/dt_corrected.json")
        eval = AutoCorrEvaluator(anno_gt_file=gt_file, anno_dt_file=dt_file)
        eval_iou = np.arange(0.05, 1.0, 0.05).tolist()

        # Initialize a dictionary to store the results for each IoU threshold
        ap, ar = {}, {}
        ap_cat, ar_cat = {}, {}

        for iou in eval_iou:
            # Save standard output
            with open(os.devnull, 'w') as devnull:
                old_stdout = sys.stdout
                sys.stdout = devnull
                try:
                    result = eval.evaluate(iou_type='segm', iou_Thrs=[iou])
                finally:
                    # Restore standard output
                    sys.stdout = old_stdout
            ap[f'{iou:.2f}'] = result.stats[0]  # Store the AP in the results dictionary
            ar[f'{iou:.2f}'] = result.stats[8]  # Store the AR
            ap_cat[f'AP@{iou:.2f}IOU'] = result.category_stats[:][0]  # Store the AP in the results dictionary
            ar_cat[f'AR@{iou:.2f}IOU'] = result.category_stats[:][8]  # Store the AR
        
        # Convert the dictionary to a DataFrame
        df_result = pd.DataFrame.from_dict(ap, orient='index')
        df_result = df_result.merge(right=pd.DataFrame.from_dict(ar, orient='index'), left_index=True, right_index=True)
        df_result.columns = ['AP', 'AR']
        df_result.insert(0, 'IOU', df_result.index)
        df_cat = pd.DataFrame.from_dict(ap_cat, orient='index').T
        df_cat = df_cat.merge(right=pd.DataFrame.from_dict(ar_cat, orient='index').T, left_index=True, right_index=True)
        category = self.createDF(self.gt_file)[0]
        df_cat.insert(0, 'Category', category['name'])
        return df_result, df_cat
    
    def export_output(self):
        
        logger = self.logger
        logger.info("Start evaluation")
        # Align metadata of dt_file with gt_file
        self.align_metadata()

        # Evaluate and get traditional and IoU range results
        trad_result = self.traditional_evaluate()
        iou_result, iou_cat = self.rangeIoU_evaluate()

        # Output the DataFrame as an excel file
        filename = self.end_eval_file
        with pd.ExcelWriter(filename) as writer:  
            trad_result.to_excel(writer, sheet_name='conventional', index=False)
            iou_result.to_excel(writer, sheet_name='overall_by_iou', index=False)
            iou_cat.to_excel(writer, sheet_name='category_by_iou', index=False)

