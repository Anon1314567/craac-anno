{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate inferrence with ground truth\n",
    "\n",
    "After inferrence by Mask-RCNN or YOLOv7, convert annotations into coco json format with annotations.ipynb\n",
    "\n",
    "Import the inferred annotation into an annotation software (CVAT by default) to review results. Add/amend/delete segmented masks as necessary.\n",
    "\n",
    "Output the reviewed results in COCO json format. The reviewed results can now be used as the \"ground truth\" to compare with the inferred annotation.\n",
    "\n",
    "Utility codes for evaluation obtained from https://github.com/cocodataset/cocoapi/issues/426 and the library pycocotools is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "__all__ = ['COCOEvaluator']\n",
    "\n",
    "class COCOEvaluator(object):\n",
    "\n",
    "    def __init__(self, anno_gt_file, anno_dt_file):\n",
    "        self.coco_gt = COCO(anno_gt_file)\n",
    "        # self.coco_dt = self.coco_gt.loadRes(anno_dt_file)\n",
    "        self.coco_dt = COCO(anno_dt_file)\n",
    "        self._hack_coco_dt()\n",
    "\n",
    "    def _hack_coco_dt(self):\n",
    "        # inferred file from Mask-R-CNN has score. \n",
    "        # YOLOv7 doesn't support exporting the score in annotation files (although the score is included in the prediction tensor det[:,4] in predict.py)\n",
    "        if 'score' in self.coco_dt.dataset['annotations']: pass\n",
    "        else:\n",
    "            for ann in self.coco_dt.dataset['annotations']:\n",
    "                ann['score'] = 1.0\n",
    "        \n",
    "        # the ground truths (after editing in CVAT) don't have scores\n",
    "        for anno in self.coco_gt.dataset['annotations']:\n",
    "            anno['score'] = 1.0\n",
    "\n",
    "    def evaluate(self, iou_type='segm', iou_Thrs=[]):\n",
    "        coco_eval = COCOeval(self.coco_gt, self.coco_dt, iou_type)\n",
    "        if iou_Thrs:\n",
    "            coco_eval.params.iouThrs = iou_Thrs\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "        coco_eval.summarize_per_category()\n",
    "        return coco_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for annotation file corrections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def findCategory(data):\n",
    "    # find categories\n",
    "    cats = data[\"categories\"]\n",
    "    category = pd.DataFrame(cats)\n",
    "    category = category.drop(['supercategory'], axis=1)\n",
    "    category = category.rename(columns={'id': 'category_id'})\n",
    "    return category\n",
    "\n",
    "def findImages(data):\n",
    "    img = data[\"images\"]\n",
    "    images = pd.DataFrame(img)\n",
    "    \n",
    "    # unwanted columns exist if exported from CVAT. Not if generated by my code\n",
    "    if set(['license','flickr_url','coco_url','date_captured']).issubset(images.columns):\n",
    "        images = images.drop(columns=['license','flickr_url','coco_url','date_captured'])\n",
    "    \n",
    "    return images\n",
    "\n",
    "def findAnnotations(data):\n",
    "    anno = data[\"annotations\"]\n",
    "    df = pd.DataFrame(anno)\n",
    "    return df\n",
    "\n",
    "# convert all np.integer, np.floating and np.ndarray into json recognisable int, float and lists\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# Creating and sorting the dataframe\n",
    "def createDF(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "        category = findCategory(data)\n",
    "        images = findImages(data)\n",
    "        nos_image = images['id'].max()\n",
    "        df = findAnnotations(data)\n",
    "        df = df.merge(images[['id','file_name']], left_on='image_id', right_on='id')\n",
    "        df = df.rename(columns={'id_x': 'id'})\n",
    "        df = drop_columns_if_exist(df,columns=['iscrowd','attributes','id_y'])\n",
    "        return category, images, df\n",
    "\n",
    "def drop_columns_if_exist(df, columns):\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=col)\n",
    "    return df\n",
    "\n",
    "def mergeDF(df, image, image_new):\n",
    "    # map image_id to the image df\n",
    "    image = image.merge(image_new[['id', 'file_name']], on='file_name', how='left')        \n",
    "    \n",
    "    df = df.merge(image[['id_x', 'id_y']], left_on='image_id', right_on='id_x', how='left')\n",
    "    df = df.drop(columns=['image_id', 'id_x']).rename(columns={'id_y': 'image_id'})\n",
    "\n",
    "    # Make good the dfs\n",
    "    df['iscrowd'] = 0\n",
    "    df['attributes'] = [{'occluded':False}] * len(df['id'])\n",
    "    df = df.drop(columns=['file_name'])\n",
    "    return df\n",
    "\n",
    "def fix_image_id(image1, image2, df, df2):\n",
    "    \n",
    "    image_comb = pd.concat([image1, image2], ignore_index = True)                              # combine image 1 and 2\n",
    "    # sort and find unique image names\n",
    "    image_comb = image_comb.sort_values(by=['file_name']).reset_index(drop=True)               # sort by image name\n",
    "    image_new = image_comb.drop_duplicates(subset=['file_name'])                               # Get unique image names\n",
    "    image_new = image_new.reset_index(drop=True)                                               # reset index\n",
    "    image_new['id'] = image_new.index + 1                                                      # create new image id\n",
    "\n",
    "    df = mergeDF(df, image1, image_new)\n",
    "    df2 = mergeDF(df2, image2, image_new)\n",
    "\n",
    "    return image_new, df, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files to be evaluated\n",
    "gt_file = './input/A12AL_val4_SS4.json'\n",
    "dt_file = './input/Corr3_00.json'\n",
    "\n",
    "# \"image_id\" won't match because CVAT (gt_file) outputs all images. My inference (dt_file) only outputs images with positive annotations\n",
    "# Evaluations are based on the number of images reviewed - hence the nos_image of gt_file\n",
    "# Task here: Correct image_id of the DETECTION FILE (dt_file)\n",
    "\n",
    "# Store categories, images and annotations in separate dataframes\n",
    "category, images, df = createDF(gt_file)\n",
    "category2, images2, df2 = createDF(dt_file)\n",
    "\n",
    "# Check categories\n",
    "for i in range(len(category['name'])):\n",
    "    if category['name'][i] != category2['name'][i]:\n",
    "        print('category id: {} , {} in file 1 different from category id: {} , {} in file 2. Please check'.format(category['category_id'][i], category['name'][i], category2['category_id'][i], category2['name'][i]))\n",
    "# clean category for json dump\n",
    "category = category.rename(columns={'category_id': 'id'})\n",
    "category['supercategory'] = \"\"\n",
    "\n",
    "# Change image_id in dt_file to the one of gt_file\n",
    "images_new, df, df2 = fix_image_id(images, images2, df, df2)\n",
    "\n",
    "# JSON with revised image_id exported for evaluation\n",
    "dict_to_gt = {\n",
    "    \"categories\": category.to_dict('records'),\n",
    "    \"images\": images_new.to_dict('records'),\n",
    "    \"annotations\": df.to_dict('records')\n",
    "    }\n",
    "dict_to_dt = {\n",
    "    \"categories\": category.to_dict('records'),\n",
    "    \"images\": images_new.to_dict('records'),\n",
    "    \"annotations\": df2.to_dict('records')\n",
    "    }\n",
    "with open(\"./input/gt_corrected.json\", \"w\") as outfile:\n",
    "    json.dump(dict_to_gt, outfile, cls=NpEncoder)\n",
    "with open(\"./input/dt_corrected.json\", \"w\") as outfile:\n",
    "    json.dump(dict_to_dt, outfile, cls=NpEncoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional IOU evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.25s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.075\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.037\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.067\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.080\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.080\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.110\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.082\n"
     ]
    }
   ],
   "source": [
    "gt_file = './input/gt_corrected.json'\n",
    "dt_file = './input/dt_corrected.json'\n",
    "\n",
    "eval = COCOEvaluator(anno_gt_file=gt_file, anno_dt_file=dt_file)\n",
    "# eval.iouThrs = 0.1\n",
    "# np.arange(0.05, 1.0, 0.05).tolist() # change to your desired thresholds\n",
    "result = eval.evaluate()\n",
    "\n",
    "# Paste results into a DataFrame\n",
    "# Get titles of metrics\n",
    "metrics = list([\"AP\",\"AP@.50IOU\",\"AP@.75IOU\",\"AP (small)\",\"AP (medium)\",\"AP (large)\",\"AR@1\",\"AR@10\",\"AR@100\",\"AR@100 (small)\",\"AR@100 (medium)\",\"AR@100 (large)\"])\n",
    "# with open('./original/metrics.csv', 'r', encoding='utf-8-sig') as file:\n",
    "#     metrics = file.readline().strip().split(',')\n",
    "\n",
    "# Assemble Dataframe\n",
    "stats = [list(i) for i in zip(*result.category_stats)]\n",
    "assessed = pd.DataFrame(stats, columns=metrics)\n",
    "# Copy categories\n",
    "assessed['category'] = category['name']\n",
    "metrics.insert(0,'category')\n",
    "assessed = assessed.reindex(columns = metrics)\n",
    "# Export Dataframe\n",
    "assessed.to_csv('evaluated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a range of IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "gt_file = './input/gt_corrected.json'\n",
    "dt_file = './input/dt_corrected.json'\n",
    "\n",
    "eval = COCOEvaluator(anno_gt_file=gt_file, anno_dt_file=dt_file)\n",
    "eval_iou = np.arange(0.05, 1.0, 0.05).tolist()\n",
    "\n",
    "# Initialize a dictionary to store the results for each IoU threshold\n",
    "ap, ar = {}, {}\n",
    "ap_cat, ar_cat = {}, {}\n",
    "\n",
    "for iou in eval_iou:\n",
    "    # eval.iouThrs = [iou]  # Set the IoU threshold to the current value\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            result = eval.evaluate(iou_type='segm', iou_Thrs=[iou])\n",
    "        finally:\n",
    "            # Restore standard output\n",
    "            sys.stdout = old_stdout\n",
    "    ap[f'{iou:.2f}'] = result.stats[0]  # Store the AP in the results dictionary\n",
    "    ar[f'{iou:.2f}'] = result.stats[8]  # Store the AR\n",
    "    ap_cat[f'AP@{iou:.2f}IOU'] = result.category_stats[:][0]  # Store the AP in the results dictionary\n",
    "    ar_cat[f'AR@{iou:.2f}IOU'] = result.category_stats[:][8]  # Store the AR\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_result = pd.DataFrame.from_dict(ap, orient='index')\n",
    "df_result = df_result.merge(right=pd.DataFrame.from_dict(ar, orient='index'), left_index=True, right_index=True)\n",
    "df_result.columns = ['AP', 'AR']\n",
    "df_result.insert(0, 'IOU', df_result.index)\n",
    "df_cat = pd.DataFrame.from_dict(ap_cat, orient='index').T\n",
    "df_cat = df_cat.merge(right=pd.DataFrame.from_dict(ar_cat, orient='index').T, left_index=True, right_index=True)\n",
    "df_cat.insert(0, 'Category', category['name'])\n",
    "\n",
    "# Output the DataFrame as an excel file\n",
    "with pd.ExcelWriter('output.xlsx') as writer:  \n",
    "    df_result.to_excel(writer, sheet_name='overall', index=False)\n",
    "    df_cat.to_excel(writer, sheet_name='category', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IOU</th>\n",
       "      <th>AP</th>\n",
       "      <th>AR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.265705</td>\n",
       "      <td>0.504141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.263901</td>\n",
       "      <td>0.501492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.260749</td>\n",
       "      <td>0.498686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.258005</td>\n",
       "      <td>0.496037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.244169</td>\n",
       "      <td>0.484117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.236310</td>\n",
       "      <td>0.475856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.35</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.228360</td>\n",
       "      <td>0.467752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.40</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.206202</td>\n",
       "      <td>0.438643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.45</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.180385</td>\n",
       "      <td>0.410741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.156995</td>\n",
       "      <td>0.382331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.142863</td>\n",
       "      <td>0.354349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.130045</td>\n",
       "      <td>0.320715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.65</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.096658</td>\n",
       "      <td>0.263060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.075198</td>\n",
       "      <td>0.205128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.057079</td>\n",
       "      <td>0.158508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.033927</td>\n",
       "      <td>0.125140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.85</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.010637</td>\n",
       "      <td>0.058564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.014365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       IOU        AP        AR\n",
       "0.05  0.05  0.265705  0.504141\n",
       "0.10  0.10  0.263901  0.501492\n",
       "0.15  0.15  0.260749  0.498686\n",
       "0.20  0.20  0.258005  0.496037\n",
       "0.25  0.25  0.244169  0.484117\n",
       "0.30  0.30  0.236310  0.475856\n",
       "0.35  0.35  0.228360  0.467752\n",
       "0.40  0.40  0.206202  0.438643\n",
       "0.45  0.45  0.180385  0.410741\n",
       "0.50  0.50  0.156995  0.382331\n",
       "0.55  0.55  0.142863  0.354349\n",
       "0.60  0.60  0.130045  0.320715\n",
       "0.65  0.65  0.096658  0.263060\n",
       "0.70  0.70  0.075198  0.205128\n",
       "0.75  0.75  0.057079  0.158508\n",
       "0.80  0.80  0.033927  0.125140\n",
       "0.85  0.85  0.010637  0.058564\n",
       "0.90  0.90  0.000834  0.014365\n",
       "0.95  0.95  0.000000  0.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Evaluation Files\n",
    "\n",
    "This utility reads the evaluation files exported from the CRAAC pipeline. Each evaluation .xlsx file contains three pages:\n",
    "* conventional: conventional *segm* metrics, by category\n",
    "* overall_by_iou: mAP and AR at IoU=[0.05:0.95]\n",
    "* category_by_iou: AP and AR at IoU=[0.05:0.95] by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, glob\n",
    "\n",
    "dir = '../output/240212_craac/eval'\n",
    "eval_header = 'eval_infer'\n",
    "export_file = 'infer_agg.xlsx'\n",
    "\n",
    "# read excel files into pd dataframes\n",
    "xlsx_files = glob.glob(os.path.join(dir,f'{eval_header}*.xlsx'))\n",
    "num_xlsx_files = len(xlsx_files)\n",
    "agg_df = pd.DataFrame(columns=['iter','IOU', 'AP', 'AR', 'AR_50:95'])\n",
    "for i, xlsx_file in enumerate(xlsx_files):\n",
    "    print(f'({i+1}/{num_xlsx_files}) Reading {os.path.basename(xlsx_file)}...')\n",
    "    df = pd.read_excel(xlsx_file, sheet_name='overall_by_iou')\n",
    "    metrics = df[df['IOU']==0.5].values.tolist()[0]\n",
    "    metrics.insert(0, i+1)                                  # insert iteration number\n",
    "    ar_5095 = df[df['IOU']>=0.5]['AR'].mean()\n",
    "    metrics.append(ar_5095)                                 # append AR_50:95 value\n",
    "    metrics_df = pd.DataFrame([metrics], columns=agg_df.columns)\n",
    "    agg_df = pd.concat([agg_df,metrics_df], ignore_index=True)\n",
    "\n",
    "agg_df = agg_df.reset_index(drop=True)\n",
    "agg_df.to_excel(os.path.join(dir, export_file), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
